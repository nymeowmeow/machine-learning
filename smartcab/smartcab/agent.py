import random
import math
import ast
import pandas as pd
import numpy as np
from environment import Agent, Environment
from planner import RoutePlanner
from simulator import Simulator

class LearningAgent(Agent):
    """ An agent that learns to drive in the Smartcab world.
        This is the object you will be modifying. """ 

    def __init__(self, env, learning=False, epsilon=1.0, alpha=0.5, optimized = False, method = 'expdecay'):
        super(LearningAgent, self).__init__(env)     # Set the agent in the evironment 
        self.planner = RoutePlanner(self.env, self)  # Create a route planner
        self.valid_actions = self.env.valid_actions  # The set of valid actions

        # Set parameters of the learning agent
        self.learning = learning # Whether the agent is expected to learn
        self.Q = dict()          # Create a Q-table which will be a dictionary of tuples
        self.epsilon = epsilon   # Random exploration factor
        self.alpha = alpha       # Learning factor

        ###########
        ## TO DO ##
        ###########
        # Set any additional class parameters as needed
	self.optimized = optimized
        self.trialcount = 0
	self.method = method

    def reset(self, destination=None, testing=False):
        """ The reset function is called at the beginning of each trial.
            'testing' is set to True if testing trials are being used
            once training trials have completed. """

        # Select the destination as the new location to route to
        self.planner.route_to(destination)
        
        ########### 
        ## TO DO ##
        ###########
        # Update epsilon using a decay function of your choice
        # Update additional class parameters as needed
        # If 'testing' is True, set epsilon and alpha to 0
        self.trialcount += 1
	if testing:
	    self.epsilon = 0
	    self.alpha = 0
	else:
	    if self.optimized:
		if self.method == 'decay':
	            self.epsilon = 1.0/(self.trialcount**2)
                elif self.method == 'cos':
                    self.epsilon = math.cos(self.alpha*self.trialcount)
                elif self.method == 'exp':
                    self.epsilon = self.alpha ** self.trialcount
                else:
                    self.epsilon = math.exp(-1.0*self.alpha*self.trialcount)
	    else:
	        self.epsilon = self.epsilon - 0.05

        return None

    def build_state(self):
        """ The build_state function is called when the agent requests data from the 
            environment. The next waypoint, the intersection inputs, and the deadline 
            are all features available to the agent. """

        # Collect data about the environment
        waypoint = self.planner.next_waypoint() # The next waypoint'
        inputs = self.env.sense(self)           # Visual input - intersection light and traffic
        deadline = self.env.get_deadline(self)  # Remaining deadline

        ########### 
        ## TO DO ##
        ###########
        
        # NOTE : you are not allowed to engineer eatures outside of the inputs available.
        # Because the aim of this project is to teach Reinforcement Learning, we have placed 
        # constraints in order for you to learn how to adjust epsilon and alpha, and thus learn about the balance between exploration and exploitation.
        # With the hand-engineered features, this learning process gets entirely negated.
        
        # Set 'state' as a tuple of relevant data for the agent        
        state = (inputs['light'], waypoint, inputs['left'], inputs['oncoming'])

        return state


    def get_maxQ(self, state):
        """ The get_max_Q function is called when the agent is asked to find the
            maximum Q-value of all actions based on the 'state' the smartcab is in. """

        ########### 
        ## TO DO ##
        ###########
        # Calculate the maximum Q-value of all actions for a given state

        maxQ = max(self.Q[state].values())

        return maxQ 


    def createQ(self, state):
        """ The createQ function is called when a state is generated by the agent. """

        ########### 
        ## TO DO ##
        ###########
        # When learning, check if the 'state' is not in the Q-table
        # If it is not, create a new dictionary for that state
        #   Then, for each action available, set the initial Q-value to 0.0

        if self.learning:
	    if self.Q.get(state) is None:
	        self.Q[state] = { None : 0.0, 'forward' : 0.0, 'left' : 0.0, 'right' : 0.0 }

    def choose_action(self, state):
        """ The choose_action function is called when the agent is asked to choose
            which action to take, based on the 'state' the smartcab is in. """

        # Set the agent state and default action
        self.state = state
        self.next_waypoint = self.planner.next_waypoint()

        ########### 
        ## TO DO ##
        ###########
        # When not learning, choose a random action
        # When learning, choose a random action with 'epsilon' probability
        # Otherwise, choose an action with the highest Q-value for the current state
        # Be sure that when choosing an action with highest Q-value that you randomly select between actions that "tie".
        if self.learning:
	    if self.epsilon > random.random():
	        action = random.choice(self.valid_actions)
	    else:
		maxQ = self.get_maxQ(state)
                possibleActions = [ k for (k,v) in self.Q[state].items() if v >= maxQ]
                action = random.choice(possibleActions) if len(possibleActions) > 1 else possibleActions[0]
        else:
            action = random.choice(self.valid_actions)
        return action


    def learn(self, state, action, reward):
        """ The learn function is called after the agent completes an action and
            receives a reward. This function does not consider future rewards 
            when conducting learning. """

        ########### 
        ## TO DO ##
        ###########
        # When learning, implement the value iteration update rule
        #   Use only the learning rate 'alpha' (do not use the discount factor 'gamma')

        if self.learning:
	    self.Q[state][action] = (1. - self.alpha)*self.Q[state][action] + self.alpha*reward


    def update(self):
        """ The update function is called when a time step is completed in the 
            environment for a given trial. This function will build the agent
            state, choose an action, receive a reward, and learn if enabled. """

        state = self.build_state()          # Get current state
        self.createQ(state)                 # Create 'state' in Q-table
        action = self.choose_action(state)  # Choose an action
        reward = self.env.act(self, action) # Receive a reward
        self.learn(state, action, reward)   # Q-learn

        return
        

def run(alpha = 0.5, epsilon = 1.0, ntest = 10, display = True, optimized = False, tolerance = 0.05, method = 'expdecay'):
    """ Driving function for running the simulation. 
        Press ESC to close the simulation, or [SPACE] to pause the simulation. """

    ##############
    # Create the environment
    # Flags:
    #   verbose     - set to True to display additional output from the simulation
    #   num_dummies - discrete number of dummy agents in the environment, default is 100
    #   grid_size   - discrete number of intersections (columns, rows), default is (8, 6)
    env = Environment()
    
    ##############
    # Create the driving agent
    # Flags:
    #   learning   - set to True to force the driving agent to use Q-learning
    #    * epsilon - continuous value for the exploration factor, default is 1
    #    * alpha   - continuous value for the learning rate, default is 0.5
    agent = env.create_agent(LearningAgent, learning = True, alpha = alpha, epsilon = epsilon, optimized = optimized)
    
    ##############
    # Follow the driving agent
    # Flags:
    #   enforce_deadline - set to True to enforce a deadline metric
    env.set_primary_agent(agent, enforce_deadline = True)

    ##############
    # Create the simulation
    # Flags:
    #   update_delay - continuous time (in seconds) between actions, default is 2.0 seconds
    #   display      - set to False to disable the GUI if PyGame is enabled
    #   log_metrics  - set to True to log trial and simulation results to /logs
    #   optimized    - set to True to change the default log file name
    sim = Simulator(env, update_delay = 0.01, log_metrics=True, optimized = True, display = display)
    
    ##############
    # Run the simulator
    # Flags:
    #   tolerance  - epsilon tolerance before beginning testing, default is 0.05 
    #   n_test     - discrete number of testing trials to perform, default is 0
    sim.run(n_test = ntest, tolerance = tolerance)
    return agent.trialcount, agent.Q

#having some problem referring to visuals.py running agent.py, so
#cut and paste the code here for calculate_safety and calculate_reliability
def calculate_safety(data):
        """ Calculates the safety rating of the smartcab during testing. """

        good_ratio = data['good_actions'].sum() * 1.0 / \
        (data['initial_deadline'] - data['final_deadline']).sum()

        if good_ratio == 1: # Perfect driving
                return ("A+", "green")
        else: # Imperfect driving
                if data['actions'].apply(lambda x: ast.literal_eval(x)[4]).sum() > 0: # Major accident
                        return ("F", "red")
                elif data['actions'].apply(lambda x: ast.literal_eval(x)[3]).sum() > 0: # Minor accident
                        return ("D", "#EEC700")
                elif data['actions'].apply(lambda x: ast.literal_eval(x)[2]).sum() > 0: # Major violation
                        return ("C", "#EEC700")
                else: # Minor violation
                        minor = data['actions'].apply(lambda x: ast.literal_eval(x)[1]).sum()
                        if minor >= len(data)/2: # Minor violation in at least half of the trials
                                return ("B", "green")
                        else:
                                return ("A", "green")

def calculate_reliability(data):
        """ Calculates the reliability rating of the smartcab during testing. """

        success_ratio = data['success'].sum() * 1.0 / len(data)

        if success_ratio == 1: # Always meets deadline
                return ("A+", "green")
        else:
                if success_ratio >= 0.90:
                        return ("A", "green")
                elif success_ratio >= 0.80:
                        return ("B", "green")
                elif success_ratio >= 0.70:
                        return ("C", "#EEC700")
                elif success_ratio >= 0.60:
                        return ("D", "#EEC700")
                else:
                        return ("F", "red")

def score(rating):
    #A+ assign a score of -1
    if rating == 'A+':
        return -1
    return ord(rating) - ord('A')

if __name__ == '__main__':
    random.seed(0)
    epsilon = 1.0
    ntest = 100
    epsilon = 1.0
    optimized = True

    best_safety = 1000
    best_reliability = 1000
    bestmethod = None
    bestalpha = None
    result = {}
    #for method in [ 'decay', 'cos', 'exp', 'expdecay' ]:
    for method in ['exp']:
        #for alpha in np.arange(0.01, 0.11, 0.01):
        for alpha in [0.01]:
            trialcount, q = run(alpha = alpha,  epsilon = 1.0,  ntest = ntest, 
		display = False, optimized = optimized, method = method,
                tolerance = 0.01)
            print 'Q: ', q
            df = pd.read_csv('logs/sim_improved-learning.csv')
            df = df[df['testing'] == True]
            df['good_actions'] = df['actions'].apply(lambda x: ast.literal_eval(x)[0])
            safety_rating, _ = calculate_safety(df)
            safety_score = score(safety_rating)
            reliability_rating, _ = calculate_reliability(df)
            reliability_score = score(reliability_rating)
            result[(method, alpha)] = (safety_score, reliability_score, trialcount)
            if reliability_score <= best_reliability and \
                safety_score <= best_safety:
                best_safety = safety_score
                best_reliability = reliability_score
                bestmethod = method
                bestalpha = alpha
    print 'best method: ', bestmethod, ', alpha: ', bestalpha, ', safety: ', \
          best_safety, ', reliability: ', best_reliability
    for key, value in result.items():
        print key, value
